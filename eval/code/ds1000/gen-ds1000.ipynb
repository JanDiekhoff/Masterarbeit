{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"llama.v1.0\":\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    \"llama.v1.1\":\"TinyLlama/TinyLlama_v1.1\",\n",
    "    \"llama.math\":\"TinyLlama/TinyLlama_v1.1_math_code\",\n",
    "    \"v1.0\":\"JanDkff/TinyFuncCoder-v1.0\",\n",
    "    \"v1.1\":\"JanDkff/TinyFuncCoder-v1.1\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_finetuned = False\n",
    "\n",
    "reformatted_prompt = True\n",
    "reformatted_path = \"R\" if reformatted_prompt else \"\"\n",
    "\n",
    "\n",
    "eval_types=[\"humaneval\",\"mbpp\",\"mbppplus\",\"multiple-sh\",\"multiple-cpp\",\"multiple-cs\",\"multiple-js\",\"multiple-java\",\"multiple-php\",\"multiple-ts\"] #,\"humanevalsynthesize-python\",\"humanevalsynthesize-cpp\",\"humanevalsynthesize-js\",\"humanevalsynthesize-java\",\"ds1000-all-completion\"]\n",
    "\n",
    "names = [\"v1.0\",\"v1.1\",\"llama.v1.0\",\"llama.v1.1\",\"llama.math\"]\n",
    "\n",
    "eval_types = [\"ds1000-all-completion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_problem(problem):\n",
    "    p = problem[\"prompt\"]\n",
    "    doc = p.split(\"<code>\")[0]\n",
    "    func = \"<code>\"+ \"<code>\".join(p.split(\"<code>\")[1:])\n",
    "    prompt = \"# <func> # Python\\n\"\n",
    "    for line in doc.split(\"\\n\"):\n",
    "        prompt += f\"# {line}\\n\"\n",
    "    prompt += func\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DS-1000\n",
    "from transformers import AutoModelForCausalLM, pipeline, AutoTokenizer\n",
    "from peft import PeftModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "data = load_dataset(\"xlangai/DS-1000\",split=\"test\")\n",
    "\n",
    "for name in names:\n",
    "    if name.find(\"llama\") >= 0:\n",
    "        model = AutoModelForCausalLM.from_pretrained(name_map[name])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_map[name])\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(name_map[f\"llama.{name}\"])\n",
    "        peft = PeftModelForCausalLM.from_pretrained(model=model,model_id=name_map[name])\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name_map[f\"llama.{name}\"])\n",
    "        model = peft.merge_and_unload()\n",
    "    generator = pipeline(task=\"text-generation\",model=model,tokenizer=tokenizer, max_new_tokens=512,device=\"cuda\")\n",
    "    \n",
    "    new_data = []\n",
    "    i = 1\n",
    "    for problem in data:\n",
    "        #print(problem)\n",
    "        p = reformat_problem(problem)\n",
    "        if not i%100:\n",
    "            print(f'{name}: {i}/1000')\n",
    "            print(p)\n",
    "        problem[\"output\"] = generator(p,temperature=0.2,top_p=0.95,top_k=0,num_return_sequences=1,do_sample=True)[0][\"generated_text\"]\n",
    "        new_data.append(problem)\n",
    "        i += 1\n",
    "    \n",
    "    with open(f'R_{name}_ds1000-all-completion.jsonl', 'w') as file:\n",
    "        for entry in new_data:\n",
    "            json_line = json.dumps(entry)\n",
    "            file.write(json_line + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
