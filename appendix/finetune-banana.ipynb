{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T22:13:08.835582Z",
     "iopub.status.busy": "2024-08-21T22:13:08.835391Z",
     "iopub.status.idle": "2024-08-21T22:13:09.910911Z",
     "shell.execute_reply": "2024-08-21T22:13:09.909983Z"
    },
    "papermill": {
     "duration": 1.079818,
     "end_time": "2024-08-21T22:13:09.913126",
     "exception": false,
     "start_time": "2024-08-21T22:13:08.833308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModelForCausalLM, PeftModel, PeftConfig\n",
    "import torch\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b309c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca2117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T22:13:09.922131Z",
     "iopub.status.busy": "2024-08-21T22:13:09.921701Z",
     "iopub.status.idle": "2024-08-21T22:13:10.750690Z",
     "shell.execute_reply": "2024-08-21T22:13:10.749748Z"
    },
    "papermill": {
     "duration": 0.83477,
     "end_time": "2024-08-21T22:13:10.751766",
     "exception": false,
     "start_time": "2024-08-21T22:13:09.916996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_configs = []\n",
    "\n",
    "for b in [False,True]:\n",
    "    for e in [False,True]:\n",
    "        for f in [ \"float32\",\"bfloat16\"]:\n",
    "            bnb_configs.append(\n",
    "                {\n",
    "                \"load_in_4bit\":b,\n",
    "                \"bnb_4bit_use_double_quant\":e,\n",
    "                \"bnb_4bit_compute_dtype\":f \n",
    "                }\n",
    "            )\n",
    "\n",
    "len(bnb_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_configs = []\n",
    "\n",
    "for a in [1024,64,8,1]:\n",
    "    for b in [1024,64,8,1]:\n",
    "        for d in [0.1,0]:\n",
    "            lora_configs.append(\n",
    "                {\n",
    "                    \"r\":a,\n",
    "                    \"lora_alpha\":b,\n",
    "                    \"lora_dropout\":d,\n",
    "                }\n",
    "            )\n",
    "                            \n",
    "len(lora_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_configs = []\n",
    "\n",
    "for c in [0.1,0]:\n",
    "    for d in [1e-5,1e-4]:\n",
    "        for f in [False,True]:\n",
    "                base_configs.append(\n",
    "                    {\n",
    "                        \"weight_decay\":c,\n",
    "                        \"learning_rate\":d,\n",
    "                        \"fp16\":f,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "len(base_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9090529",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_configs = []\n",
    "\n",
    "for a in [True, False]:\n",
    "    for b in [False,True]:\n",
    "        if not a and b: continue\n",
    "        train_configs.append(\n",
    "            {\n",
    "                \"lora\":a,\n",
    "                \"bnb\":b\n",
    "            }\n",
    "        )\n",
    "\n",
    "len(train_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eafc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(new_data, filename='data_incomplete.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "        file_data = json.load(file)\n",
    "        file_data.append(new_data)\n",
    "        file.seek(0)\n",
    "        json.dump(file_data, file, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_and_print(model,tokenizer,p=False,t=\"\"):\n",
    "    prompt = \"Hello\"\n",
    "    gen = pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\", device_map=\"auto\",max_new_tokens=10)\n",
    "    result = gen(prompt)[0][\"generated_text\"]\n",
    "    if p: print(t, \": \",result)\n",
    "    \n",
    "    del gen\n",
    "    gc.collect()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e404755",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "def train_model(base_config, lora_config, bnb_config, save=True, prt=False):\n",
    "    global i\n",
    "    i += 1\n",
    "    if i < 28: return\n",
    "    \n",
    "    model_name = \"TinyLlama/TinyLlama_v1.1\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    data = Dataset.from_json(\"data/banana.json\")\n",
    "    data = data.map(lambda example : tokenizer(example[\"text\"], return_tensors=\"np\"), batched=True)\n",
    "\n",
    "    if bnb_config:\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=bnb_config[\"load_in_4bit\"],\n",
    "            bnb_4bit_use_double_quant=bnb_config[\"bnb_4bit_use_double_quant\"],\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=bnb_config[\"bnb_4bit_compute_dtype\"]\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    revision=\"main\",\n",
    "                    quantization_config = bnb\n",
    "                )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                revision=\"main\"\n",
    "            )\n",
    "    \n",
    "    if lora_config:\n",
    "        lora = LoraConfig(\n",
    "            r = lora_config[\"r\"],\n",
    "            lora_alpha = lora_config[\"lora_alpha\"],\n",
    "            init_lora_weights = True,\n",
    "            lora_dropout = lora_config[\"lora_dropout\"],\n",
    "            bias = 'none',\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model,lora)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        fp16=base_config[\"fp16\"],\n",
    "        weight_decay=base_config[\"weight_decay\"],\n",
    "        learning_rate=base_config[\"learning_rate\"],\n",
    "        label_names=['input_ids'],\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        no_cuda=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        eval_steps=1000\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    save_path = \"tmp\"\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "    generation = {\"args\":base_config,\"lora\":lora_config,\"bnb\":bnb_config}\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=False, torch_dtype=\"bfloat16\")\n",
    "    \n",
    "    generation[\"base\"] = create_pipeline_and_print(model=base_model,tokenizer=tokenizer,t=\"base\",p=prt)\n",
    "    \n",
    "    generation[\"direct\"] = create_pipeline_and_print(model=model,tokenizer=tokenizer,t=\"direct\",p=prt)\n",
    "    \n",
    "    if lora_config:\n",
    "        merged_model = model.merge_and_unload()\n",
    "\n",
    "        generation[\"merged\"] = create_pipeline_and_print(model=merged_model,tokenizer=tokenizer,t=\"merged\",p=prt)\n",
    "    \n",
    "    \n",
    "    if lora_config:\n",
    "        loaded_model = PeftModelForCausalLM.from_pretrained(base_model,save_path)\n",
    "    else:\n",
    "        loaded_model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "    \n",
    "    generation[\"loaded\"] = create_pipeline_and_print(model=loaded_model,tokenizer=tokenizer,t=\"loaded\",p=prt)\n",
    "    \n",
    "    if lora_config:\n",
    "        merged_loaded_model = loaded_model.merge_and_unload()\n",
    "        generation[\"merged_loaded\"] = create_pipeline_and_print(model=merged_loaded_model,tokenizer=tokenizer,t=\"mergedLoaded\",p=prt)\n",
    "        \n",
    "        del loaded_model\n",
    "        del merged_loaded_model\n",
    "    \n",
    "    if save: write_json(generation)\n",
    "    \n",
    "    del tokenizer\n",
    "    del collator\n",
    "    del data\n",
    "    del args\n",
    "    del model\n",
    "    del base_model\n",
    "    del merged_model\n",
    "    del generation\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac084858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model({\"weight_decay\":0.1,\"learning_rate\":0.0001,\"fp16\":False},\n",
    "            {\"r\":1024,\"lora_alpha\":64,\"lora_dropout\":0.1},\n",
    "            {\"load_in_4bit\":True,\"bnb_4bit_use_double_quant\":True,\"bnb_4bit_compute_dtype\":\"bfloat16\"}, \n",
    "            #False,\n",
    "            prt=True,save=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"bnb = BitsAndBytesConfig(\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_use_double_quant=True,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    revision=\"main\",\n",
    "    quantization_config = bnb\n",
    ")\n",
    "\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r = 64,\n",
    "    lora_alpha = 64,\n",
    "    init_lora_weights = True,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = 'none',\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model,lora)\n",
    "    \n",
    "args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    fp16=True,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=1e-5,\n",
    "    label_names=['input_ids'],\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    no_cuda=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "save_path = \"tmp\"\n",
    "trainer.save_model(save_path)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create_pipeline_and_print(model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "create_pipeline_and_print(base_model)\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "create_pipeline_and_print(model)\n",
    "create_pipeline_and_print(base_model)\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_path)\n",
    "create_pipeline_and_print(loaded_model)\n",
    "\n",
    "peft_model = PeftModelForCausalLM.from_pretrained(base_model,save_path)\n",
    "create_pipeline_and_print(peft_model)\n",
    "\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "create_pipeline_and_print(peft_model)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for train_config in train_configs:\n",
    "    for base_config in base_configs:\n",
    "        if train_config[\"lora\"]:\n",
    "            for lora_config in lora_configs:\n",
    "                if train_config[\"bnb\"]:\n",
    "                    for bnb_config in bnb_configs:\n",
    "                        train_model(base_config, lora_config, bnb_config)\n",
    "                else:\n",
    "                    train_model(base_config, lora_config, False)\n",
    "        else:\n",
    "            train_model(base_config, False, False)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.567572,
   "end_time": "2024-08-21T22:13:14.498311",
   "environment_variables": {},
   "exception": true,
   "input_path": "finetune-llama.ipynb",
   "output_path": "finetune-llama-out.ipynb",
   "parameters": {},
   "start_time": "2024-08-21T22:13:07.930739",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
