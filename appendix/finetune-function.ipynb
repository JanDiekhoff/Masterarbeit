{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e93e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T22:13:08.835582Z",
     "iopub.status.busy": "2024-08-21T22:13:08.835391Z",
     "iopub.status.idle": "2024-08-21T22:13:09.910911Z",
     "shell.execute_reply": "2024-08-21T22:13:09.909983Z"
    },
    "papermill": {
     "duration": 1.079818,
     "end_time": "2024-08-21T22:13:09.913126",
     "exception": false,
     "start_time": "2024-08-21T22:13:08.833308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer\n",
    "import os\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModelForCausalLM\n",
    "import torch\n",
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b309c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_seed(seed)\n",
    "model_name = \"TinyLlama/TinyLlama_v1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e404755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(base_config, lora_config, bnb_config, data, tokenizer, collator):\n",
    "    global model_name\n",
    "\n",
    "    if bnb_config:\n",
    "        print(\"QLORA\")\n",
    "        bnb = BitsAndBytesConfig(\n",
    "            load_in_4bit=bnb_config[\"load_in_4bit\"],\n",
    "            bnb_4bit_use_double_quant=bnb_config[\"bnb_4bit_use_double_quant\"],\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=bnb_config[\"bnb_4bit_compute_dtype\"]\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    revision=\"main\",\n",
    "                    quantization_config = bnb\n",
    "                )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\",\n",
    "                revision=\"main\"\n",
    "            )\n",
    "    \n",
    "    if lora_config:\n",
    "        print(\"LORA\")\n",
    "        lora = LoraConfig(\n",
    "            r = lora_config[\"r\"],\n",
    "            lora_alpha = lora_config[\"lora_alpha\"],\n",
    "            init_lora_weights = True,\n",
    "            lora_dropout = lora_config[\"lora_dropout\"],\n",
    "            bias = 'none',\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model,lora)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=\".\",\n",
    "        fp16=base_config[\"fp16\"],\n",
    "        weight_decay=base_config[\"weight_decay\"],\n",
    "        learning_rate=base_config[\"learning_rate\"],\n",
    "        label_names=['input_ids'],\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=8,\n",
    "        no_cuda=False,\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    save_path = \"tmp_trainer_smol\"\n",
    "    #trainer.save_model(save_path)\n",
    "    #model.save_pretrained(save_path+\"_peft\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "def reformat_func(example):\n",
    "    example[\"full\"] = \"# <func>\\n\" + example[\"head\"] + example[\"body\"] + \"\\n</func>\"\n",
    "    return example\n",
    "\n",
    "def tokenize_func(example):\n",
    "    return tokenizer(example[\"full\"], return_tensors=\"np\",padding=\"max_length\",max_length=1000)\n",
    "\n",
    "data = Dataset.from_parquet(\"../data/chunks/chunk_1.parquet\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "filtered_df = df[df[\"language\"] == \"Python\"]\n",
    "\n",
    "sampled_df = filtered_df.sample(frac=.005, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data = Dataset.from_pandas(sampled_df)\n",
    "\n",
    "print(f\"Original size after filtering: {len(filtered_df)}\")\n",
    "print(f\"Sampled size (1%): {len(sampled_df)}\")\n",
    "print(sampled_df.head())\n",
    "\n",
    "\n",
    "data = data.map(reformat_func)\n",
    "tokenized_ds = data.map(tokenize_func, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac084858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model({\"weight_decay\":0.1,\"learning_rate\":1e-4,\"fp16\":False},\n",
    "            {\"r\":4096,\"lora_alpha\":4096,\"lora_dropout\":0.1},\n",
    "            {\"load_in_4bit\":True,\"bnb_4bit_use_double_quant\":True,\"bnb_4bit_compute_dtype\":\"bfloat16\"},\n",
    "            #False,\n",
    "            tokenized_ds,tokenizer,collator)\n",
    "\n",
    "prompt = \"\"\"from typing import List\\n# <func>\\n# Python\\n# Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n#>>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n# False\\n# >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n# True\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\"\"\"\n",
    "\n",
    "gen = pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\", device_map=\"auto\",max_new_tokens=512)\n",
    "print(gen(prompt))\n",
    "\n",
    "\n",
    "gen = pipeline(model=model.merge_and_unload(), tokenizer=tokenizer, task=\"text-generation\", device_map=\"auto\",max_new_tokens=512)\n",
    "print(gen(prompt))\n",
    "\n",
    "\n",
    "gen = pipeline(model=AutoModelForCausalLM.from_pretrained(model_name), tokenizer=tokenizer, task=\"text-generation\", device_map=\"auto\",max_new_tokens=512)\n",
    "print(gen(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.567572,
   "end_time": "2024-08-21T22:13:14.498311",
   "environment_variables": {},
   "exception": true,
   "input_path": "finetune-llama.ipynb",
   "output_path": "finetune-llama-out.ipynb",
   "parameters": {},
   "start_time": "2024-08-21T22:13:07.930739",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
