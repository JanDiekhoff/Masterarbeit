{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09c9e0e-3f62-4002-8046-b27232480f14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.032315Z",
     "iopub.status.busy": "2024-10-20T12:24:01.032021Z",
     "iopub.status.idle": "2024-10-20T12:24:01.284227Z",
     "shell.execute_reply": "2024-10-20T12:24:01.283723Z"
    },
    "papermill": {
     "duration": 0.256857,
     "end_time": "2024-10-20T12:24:01.285904",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.029047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "regex = {\n",
    "    \"PHP\":        r'(?<doc>(\\/\\*\\*(.|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>(public|protected|private)*\\s*function\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})',\n",
    "    \"JavaScript\": r'(?<doc>(\\/\\*\\*(?:(?!\\/\\*\\*).|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>function\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})',\n",
    "    \"C\":          r\"(?<doc>(\\/\\*(?:(?!\\/\\*).|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>(public|private|protected)?\\s*(static)?\\s*\\w+\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})\",\n",
    "    \"C++\":        r\"(?<doc>(\\/\\*(?:(?!\\/\\*).|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>(public|private|protected)?\\s*(static)?\\s*\\w+\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})\",\n",
    "    \"C#\":         r\"(?<doc>(\\/\\/\\/.*\\s*)+)\\s*(?<name>(public|private|protected|internal|file|protected internal|private protected)?\\s*(static)?\\s*\\w+\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})\",\n",
    "    \"Java\":       r\"(?<doc>(\\/\\*\\*(?:(?!\\/\\*\\*).|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>(public|private|protected)?\\s*(static)?\\s*\\w+\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})\",\n",
    "    \"TypeScript\": r'(?<doc>(\\/\\*\\*(?:(?!\\/\\*\\*).|\\n)*?\\*\\/|(\\/\\/.*?\\s*)+))\\s*(?<name>function\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})',\n",
    "    \"Shell\":      r\"(?<doc>(\\#.*?\\s*)+)\\s*(?<name>\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})\",\n",
    "    \"Ruby\":       r'(?<doc>(\\#.*?\\s*)+)\\s*(?<name>def\\s+\\w+)\\s*(?<paren>\\(([^()]+|(?&paren))*\\))\\s*(?<brace>\\{([^{}]+|(?&brace))*\\})',\n",
    "    \"Python\": r\"\"\n",
    "}\n",
    "\n",
    "keywords = [\"if\", \"while\", \"do\", \"else\", \"switch\", \"elseif\", \"elif\", \"match\", \"for\", \"try\", \"catch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65431092-5744-48f3-8754-0114449ef63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.289198Z",
     "iopub.status.busy": "2024-10-20T12:24:01.288886Z",
     "iopub.status.idle": "2024-10-20T12:24:01.294754Z",
     "shell.execute_reply": "2024-10-20T12:24:01.294333Z"
    },
    "papermill": {
     "duration": 0.008456,
     "end_time": "2024-10-20T12:24:01.295604",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.287148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def add_curly_brace(match):\n",
    "    return match.group(1) + \" {\"\n",
    "\n",
    "def Ruby_hotfix(col):\n",
    "    col = ruby_regex.sub(add_curly_brace, col)\n",
    "    col = ' '.join(word_map.get(word, word) for word in col.split())\n",
    "    return col\n",
    "\n",
    "\n",
    "def reverse_Ruby_hotfix(functions):\n",
    "    if functions:\n",
    "        for func in functions:\n",
    "            func[-1] = ' '.join(reverse_word_map.get(word, word) for word in func[-1].split())\n",
    "            func[2] = ' '.join(reverse_word_map.get(word, word) for word in func[2].split())\n",
    "            \n",
    "            i = func[-1].find('{')\n",
    "            if i != -1:\n",
    "                func[-1] = func[-1][:i] + func[-1][i + 1:]\n",
    "            if func[2][0] == \"{\":\n",
    "                func[2] = func[2][1:]\n",
    "            \n",
    "    return functions\n",
    "\n",
    "\n",
    "word_map = { \n",
    "    \"do\":\"do{\", \n",
    "    \"if\":\"if{\", \n",
    "    \"unless\":\"unless{\", \n",
    "    \"case\":\"case{\", \n",
    "    \"while\":\"while{\", \n",
    "    \"until\":\"until{\",\n",
    "    \"end\":\"end}\",\n",
    "    \"class\":\"class{\",\n",
    "    \"module\":\"module{\"\n",
    "}\n",
    "\n",
    "reverse_word_map = {v: k for k, v in word_map.items()}\n",
    "\n",
    "ruby_regex = re.compile(r'(def\\s+\\w+\\s*(?<paren>\\(([^()]+|(?&paren))*\\)))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cfe717-409e-4815-8a03-1c3f3f1dfb5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.298509Z",
     "iopub.status.busy": "2024-10-20T12:24:01.298341Z",
     "iopub.status.idle": "2024-10-20T12:24:01.301909Z",
     "shell.execute_reply": "2024-10-20T12:24:01.301481Z"
    },
    "papermill": {
     "duration": 0.006029,
     "end_time": "2024-10-20T12:24:01.302744",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.296715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "whitespace = re.compile(r'\\s+')\n",
    "def extract_functions(code):\n",
    "    # Regular expression pattern to match functions\n",
    "    \n",
    "    functions = []\n",
    "    matches = re.finditer(pattern, code)\n",
    "        \n",
    "    for match in matches:\n",
    "        docstring = match.group(\"doc\")\n",
    "        name = match.group(\"name\")\n",
    "        parameters = match.group(\"paren\")\n",
    "        body = match.group(\"brace\").strip()\n",
    "        functions.append([name, parameters, body, docstring])\n",
    "        \n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7278d72-e3da-409c-bfd1-4fac47ade063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.305993Z",
     "iopub.status.busy": "2024-10-20T12:24:01.305754Z",
     "iopub.status.idle": "2024-10-20T12:24:01.308702Z",
     "shell.execute_reply": "2024-10-20T12:24:01.308165Z"
    },
    "papermill": {
     "duration": 0.005389,
     "end_time": "2024-10-20T12:24:01.309550",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.304161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_function(example):\n",
    "    return example['max_stars_count'] and example['max_stars_count'] > 2 and example['lang'] in regex.keys() and example['size'] < 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99e3b1a-d8c5-4f8a-b336-9756ee78de91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.312807Z",
     "iopub.status.busy": "2024-10-20T12:24:01.312501Z",
     "iopub.status.idle": "2024-10-20T12:24:01.317278Z",
     "shell.execute_reply": "2024-10-20T12:24:01.316661Z"
    },
    "papermill": {
     "duration": 0.007459,
     "end_time": "2024-10-20T12:24:01.318164",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.310705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_data(example, df):\n",
    "    functions_data = []\n",
    "    if filter_function(example):\n",
    "        lang = example['lang']\n",
    "        \n",
    "        if lang == \"Ruby\":\n",
    "            example['content'] = Ruby_hotfix(example['content'])\n",
    "        \n",
    "        functions = extract_functions(example['content'])\n",
    "        \n",
    "        if language == \"Ruby\":\n",
    "            functions = reverse_Ruby_hotfix(functions)\n",
    "        \n",
    "        if functions:\n",
    "            for function in functions:\n",
    "                if function:\n",
    "                    if re.sub(whitespace, '', function[0]) in keywords: continue\n",
    "                    # if len(function[2]) > 1000: continue\n",
    "                    if not function[2].replace('{','').replace('}','').replace(':','').replace('\\n','').strip(): continue\n",
    "                    if not function[3] or (function[3] and not function[3].replace(\"//\",\"\").replace(\"///\",\"\").replace(\"#\",\"\").replace(\"//*\",\"\").replace(\"*/\",\"\").replace(\"'''\",\"\").replace('\"\"\"',\"\").strip()): continue\n",
    "                    \n",
    "                    functions_data.append({\n",
    "                        'name':function[0],\n",
    "                        'params':function[1],\n",
    "                        'body':function[2],\n",
    "                        'docstring':function[3],\n",
    "                        #'full_code':function[4],\n",
    "                        'file_id':example['hexsha'],\n",
    "                        'language':lang,\n",
    "                    })\n",
    "    return functions_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4365da-a252-4668-ac39-62912d8ab3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.321287Z",
     "iopub.status.busy": "2024-10-20T12:24:01.321128Z",
     "iopub.status.idle": "2024-10-20T12:24:01.327497Z",
     "shell.execute_reply": "2024-10-20T12:24:01.326921Z"
    },
    "papermill": {
     "duration": 0.008971,
     "end_time": "2024-10-20T12:24:01.328321",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.319350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Function to extract function definitions from Python code\n",
    "def extract_python_functions(code, error_counter, hexsha):\n",
    "    functions = []\n",
    "    try:\n",
    "        code = code.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "        # Parse the code into an AST\n",
    "        tree = ast.parse(code)\n",
    "        # Iterate over all nodes in the AST\n",
    "        for node in ast.walk(tree):\n",
    "            # Check if the node is a function definition\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                # Extract the function name and its definition\n",
    "                function_name = node.name.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "                params = str([arg.arg.encode('utf-8', 'ignore').decode('utf-8', 'ignore') for arg in node.args.args])\n",
    "                function_code = ast.get_source_segment(code, node).encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "                docstring = ast.get_docstring(node)\n",
    "                if docstring: docstring = docstring.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "                body = \"\\n\".join([ast.unparse(stmt) for stmt in node.body]).encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "                if hexsha: hexsha = hexsha.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "                \n",
    "                functions.append([\n",
    "                    function_name,\n",
    "                    params,\n",
    "                    body,\n",
    "                    docstring,\n",
    "                    function_code,\n",
    "                    hexsha\n",
    "                    ])\n",
    "    except (SyntaxError, ValueError, UnicodeEncodeError, UnicodeDecodeError):\n",
    "        error_counter += 1\n",
    "    return functions, error_counter\n",
    "\n",
    "        \n",
    "def handle_py_data(example, df, py_error_count):\n",
    "    functions_data = []\n",
    "    if filter_function(example):\n",
    "        #print(example, flush=True)\n",
    "        hexsha = example['hexsha']\n",
    "        functions, py_error_count = extract_python_functions(example['content'], py_error_count, hexsha)\n",
    "\n",
    "        if functions:\n",
    "            for function in functions:\n",
    "                if function:\n",
    "                    functions_data.append({\n",
    "                        'name':function[0],\n",
    "                        'params':function[1],\n",
    "                        'body':function[2],\n",
    "                        'docstring':function[3],\n",
    "                        #'full_code':function[4],\n",
    "                        'file_id': function[5],\n",
    "                        'language':'Python',\n",
    "                    })\n",
    "    return functions_data, py_error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3640291-b683-4f82-9d06-9f0180ae623c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:01.331504Z",
     "iopub.status.busy": "2024-10-20T12:24:01.331369Z",
     "iopub.status.idle": "2024-10-20T12:24:04.651211Z",
     "shell.execute_reply": "2024-10-20T12:24:04.650161Z"
    },
    "papermill": {
     "duration": 3.323899,
     "end_time": "2024-10-20T12:24:04.653531",
     "exception": false,
     "start_time": "2024-10-20T12:24:01.329632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\User\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1029713b-0310-41f7-a950-01b3f60a21a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T12:24:04.662141Z",
     "iopub.status.busy": "2024-10-20T12:24:04.661900Z",
     "iopub.status.idle": "2024-10-20T12:24:04.666657Z",
     "shell.execute_reply": "2024-10-20T12:24:04.665903Z"
    },
    "papermill": {
     "duration": 0.011173,
     "end_time": "2024-10-20T12:24:04.668562",
     "exception": false,
     "start_time": "2024-10-20T12:24:04.657389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(data_list):\n",
    "    dict_of_lists = {}\n",
    "    for d in data_list:\n",
    "        for key, value in d.items():\n",
    "            if key not in dict_of_lists:\n",
    "                dict_of_lists[key] = []\n",
    "            dict_of_lists[key].append(value)\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f7460-a138-49e9-b67b-2bfcccf93be7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-10-20T12:24:04.672145",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 544 minutes +\n",
    "\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "chunk_size = 1000000\n",
    "hf_repo_id = \"JanDkff/TinyFuncData-new\"\n",
    "api = HfApi()\n",
    "token = HfFolder.get_token()\n",
    "\n",
    "#expected_features = Features({\n",
    "#    'name': Value(dtype='string'),\n",
    "#    'params': Value(dtype='string'),\n",
    "#    'body': Value(dtype='string'),\n",
    "#    'docstring': Value(dtype='string'),\n",
    "#    'file_id': Value(dtype='string'),\n",
    "#    'language': Value(dtype='string')\n",
    "#})\n",
    "\n",
    "# Process the dataset in chunks\n",
    "processed_data = []\n",
    "\n",
    "for language in regex.keys():\n",
    "    i = 0\n",
    "    j = 0\n",
    "    start_time = time.time()\n",
    "    dataset = load_dataset('bigcode/the-stack-dedup', data_dir=f'data/{language.lower().replace(\"++\",\"pp\").replace(\"#\",\"-sharp\")}', streaming=True, split=\"train\")\n",
    "    pattern = regex[language]\n",
    "    for example in dataset:\n",
    "        if not i % 50000:\n",
    "            current_time = time.time()\n",
    "            print(i, language, round(current_time - start_time, 2), flush=True)\n",
    "            if processed_data: print(processed_data[-1])\n",
    "            #start_time = current_time\n",
    "        i += 1\n",
    "\n",
    "        processed_data += handle_data(example, dataset)\n",
    "        if len(processed_data) >= chunk_size:\n",
    "            temp_dataset = Dataset.from_dict(list_of_dicts_to_dict_of_lists(processed_data))\n",
    "            temp_dataset.push_to_hub(hf_repo_id, token=token, split=f'part_{j}_{language.lower().replace(\"++\",\"pp\").replace(\"#\",\"sharp\")}')\n",
    "            print(j)\n",
    "            j += 1\n",
    "            processed_data = []  # Reset the data chunks\n",
    "            temp_dataset = None\n",
    "\n",
    "    # Write any remaining data\n",
    "    if processed_data:\n",
    "        temp_dataset = Dataset.from_dict(list_of_dicts_to_dict_of_lists(processed_data))\n",
    "        temp_dataset.push_to_hub(hf_repo_id, token=token, split=f'part_{j}_{language.lower().replace(\"++\",\"pp\").replace(\"#\",\"sharp\")}')\n",
    "        processed_data = []\n",
    "        temp_dataset = None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "stream-data.ipynb",
   "output_path": "stream-data-out.ipynb",
   "parameters": {},
   "start_time": "2024-10-20T12:24:00.169333",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
