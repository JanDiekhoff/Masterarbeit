% This file was created with Citavi 6.19.2.1

@proceedings{.2010,
 year = {2010},
 title = {2010 International Conference on Future Information Technology and Management Engineering},
 publisher = {IEEE},
 isbn = {978-1-4244-9087-5}
}


@article{Rumelhart.1986,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 year = {1986},
 title = {Learning representations by back-propagating errors},
 url = {https://api.semanticscholar.org/CorpusID:205001834},
 pages = {533--536},
 volume = {323},
 journal = {Nature}
}


@misc{Roziere.2023,
 abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67{\%} and 65{\%} on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
 author = {Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
 date = {2023},
 title = {Code Llama: Open Foundation Models for Code},
 url = {http://arxiv.org/pdf/2308.12950},
 file = {Rozi{\`e}re, Gehring et al. 24.08.2023 - Code Llama:Attachments/Rozi{\`e}re, Gehring et al. 24.08.2023 - Code Llama.pdf:application/pdf}
}


@proceedings{Rogers.2023,
 year = {2023},
 title = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 address = {Toronto, Canada},
 publisher = {{Association for Computational Linguistics}},
 editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki}
}


@proceedings{Riloff.2018,
 year = {2018},
 title = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 address = {Brussels, Belgium},
 publisher = {{Association for Computational Linguistics}},
 editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi}
}


@article{Ren.2020,
 author = {Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
 year = {2020},
 title = {CodeBLEU: a Method for Automatic Evaluation of Code Synthesis},
 volume = {abs/2009.10297},
 journal = {CoRR}
}


@misc{QwenTeam.2024,
 author = {{Qwen Team}},
 year = {2024},
 title = {Qwen2.5: A Party of Foundation Models},
 url = {https://qwenlm.github.io/blog/qwen2.5/}
}


@proceedings{.2024,
 year = {2024},
 title = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
 address = {Red Hook, NY, USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS '23}
}


@proceedings{.2020,
 year = {2020},
 title = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
 address = {Red Hook, NY, USA},
 publisher = {{Curran Associates Inc}},
 isbn = {9781713829546},
 series = {NIPS '20}
}


@inproceedings{Papineni.2001,
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
 title = {BLEU},
 pages = {311},
 publisher = {{Association for Computational Linguistics}},
 editor = {Isabelle, Pierre},
 booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - ACL '02},
 year = {2001},
 address = {Morristown, NJ, USA},
 doi = {10.3115/1073083.1073135},
 file = {Papineni, Roukos et al. 2001 - BLEU:Attachments/Papineni, Roukos et al. 2001 - BLEU.pdf:application/pdf}
}


@misc{OpenAI.2022,
 author = {OpenAI},
 year = {2022},
 title = {Introducing ChatGPT},
 url = {https://openai.com/index/chatgpt/},
 urldate = {2024-06-06}
}


@proceedings{Oh.2023,
 year = {2023},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{Curran Associates, Inc}},
 editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.}
}


@misc{Odena.2021,
 author = {Odena, Augustus and Sutton, Charles and Dohan, David Martin and Jiang, Ellen and Michalewski, Henryk and Austin, Jacob and Bosma, Maarten Paul and Nye, Maxwell and Terry, Michael and Le, Quoc V.},
 date = {2021},
 title = {Program synthesis with large language models},
 url = {https://research.google/pubs/program-synthesis-with-large-language-models/},
 institution = {{Google Research}}
}


@proceedings{.2019,
 year = {2019},
 title = {North American Chapter of the Association for Computational Linguistics}
}


@inproceedings{Nijkamp.2022,
 author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Haiquan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
 title = {CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
 url = {https://api.semanticscholar.org/CorpusID:252668917},
 booktitle = {International Conference on Learning Representations},
 year = {2022}
}


@proceedings{Salakhutdinov.2024,
 year = {2024},
 title = {Proceedings of the 41st International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix}
}


@article{Nijkamp.2022b,
 author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
 year = {2022},
 title = {A Conversational Paradigm for Program Synthesis},
 journal = {arXiv preprint}
}


@book{.2018,
 year = {2018},
 title = {Natural language processing},
 url = {https://csunibo.github.io/natural-language-processing/books/eisenstein-natural-language-processing.pdf},
 file = {Natural language processing 2018:Attachments/Natural language processing 2018.pdf:application/pdf}
}


@article{Nadkarni.2011,
 abstract = {OBJECTIVES

To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design.

TARGET AUDIENCE

This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art.

SCOPE

We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field.},
 author = {Nadkarni, Prakash M. and Ohno-Machado, Lucila and Chapman, Wendy W.},
 year = {2011},
 title = {Natural language processing: an introduction},
 pages = {544--551},
 volume = {18},
 number = {5},
 journal = {Journal of the American Medical Informatics Association : JAMIA},
 doi = {10.1136/amiajnl-2011-000464},
 file = {Nadkarni, Ohno-Machado et al. 2011 - Natural language processing:Attachments/Nadkarni, Ohno-Machado et al. 2011 - Natural language processing.pdf:application/pdf}
}


@misc{Muennighoff.25.05.2023,
 abstract = {The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.},
 author = {Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and {Le Scao}, Teven and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin},
 date = {25.05.2023},
 title = {Scaling Data-Constrained Language Models},
 url = {http://arxiv.org/pdf/2305.16264},
 file = {Muennighoff, Rush et al. 25.05.2023 - Scaling Data-Constrained Language Models:Attachments/Muennighoff, Rush et al. 25.05.2023 - Scaling Data-Constrained Language Models.pdf:application/pdf}
}


@inproceedings{Muennighoff.2024,
 author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel Randy and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and von Werra, Leandro and Longpre, Shayne},
 title = {OctoPack: Instruction Tuning Code Large Language Models},
 url = {https://openreview.net/forum?id=mw1PWNSWZP},
 booktitle = {The Twelfth International Conference on Learning Representations},
 year = {2024}
}


@proceedings{Moschitti.2014,
 year = {2014},
 title = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 address = {Doha, Qatar},
 publisher = {{Association for Computational Linguistics}},
 editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter}
}


@misc{MojanJavaheripi.2023,
 author = {{Mojan Javaheripi}, S{\'e}bastien Bubeck},
 year = {2023},
 title = {Phi-2: The surprising power of small language models},
 url = {https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/},
 urldate = {12.04.2024}
}


@inproceedings{Mikolov.2013,
 author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
 title = {Linguistic Regularities in Continuous Space Word Representations},
 url = {https://aclanthology.org/N13-1090},
 pages = {746--751},
 publisher = {{Association for Computational Linguistics}},
 editor = {Vanderwende, Lucy and {Daum{\'e} III}, Hal and Kirchhoff, Katrin},
 booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 year = {2013},
 address = {Atlanta, Georgia}
}


@inproceedings{Mikolov.2013b,
 author = {Mikolov, Tomas and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
 title = {Efficient Estimation of Word Representations in Vector Space},
 url = {https://api.semanticscholar.org/CorpusID:5959482},
 booktitle = {International Conference on Learning Representations},
 year = {2013}
}


@article{Meyer.1992,
 author = {Meyer, B.},
 year = {1992},
 title = {Applying 'design by contract'},
 keywords = {Books;Computer bugs;Contracts;Guidelines;Object oriented programming;Pressing;Reliability theory;Robustness;Software engineering;Software systems},
 pages = {40--51},
 volume = {25},
 number = {10},
 journal = {Computer},
 doi = {10.1109/2.161279}
}


@book{Manning.1999,
 abstract = {Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.},
 author = {Manning, Christopher D. and Sch{\"u}tze, Hinrich},
 year = {1999},
 title = {Foundations of statistical natural language processing},
 address = {Cambridge Mass.},
 publisher = {{MIT Press}},
 isbn = {9780262133609}
}


@inproceedings{Luo.2024,
 author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
 title = {WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
 url = {https://openreview.net/forum?id=UnUwSIgK5W},
 booktitle = {The Twelfth International Conference on Learning Representations},
 year = {2024}
}


@article{Lu.2021,
 author = {Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin B. and Drain, Dawn and Jiang, Daxin and Tang, Duyu and Li, Ge and Zhou, Lidong and Shou, Linjun and Zhou, Long and Tufano, Michele and Gong, Ming and Zhou, Ming and Duan, Nan and Sundaresan, Neel and Deng, Shao Kun and Fu, Shengyu and Liu, Shujie},
 year = {2021},
 title = {CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding  and Generation},
 volume = {abs/2102.04664},
 journal = {CoRR}
}


@inproceedings{Liu.2024,
 abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus - a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HUMANEVAL benchmark by 80  to build HUMANEVAL+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9{\%}. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HUMANEVAL+, while none of them could on HUMANEVAL. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.},
 author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
 title = {Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation},
 publisher = {{Curran Associates Inc}},
 series = {NIPS '23},
 booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
 year = {2024},
 address = {Red Hook, NY, USA}
}


@inproceedings{Liu.2024b,
 abstract = {Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. The code is available at https://github.com/NVlabs/DoRA.},
 author = {Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
 title = {DoRA: Weight-Decomposed Low-Rank Adaptation},
 url = {https://proceedings.mlr.press/v235/liu24bn.html},
 pages = {32100--32121},
 volume = {235},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 year = {2024}
}


@proceedings{.2017,
 year = {2017},
 title = {NeurIPS 2017}
}


@proceedings{Linzen.2018,
 year = {2018},
 title = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
 address = {Brussels, Belgium},
 publisher = {{Association for Computational Linguistics}},
 editor = {Linzen, Tal and Chrupa{\l}a, Grzegorz and Alishahi, Afra}
}


@proceedings{Salakhutdinov.2024b,
 year = {2024},
 title = {Proceedings of the 41st International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix}
}


@misc{Soboleva.2023,
 author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R. and Hestness, Joel and Dey, Nolan},
 year = {2023},
 title = {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama},
 url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B}
}


@misc{Zhang.2024,
 abstract = {We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.},
 author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
 date = {2024},
 title = {TinyLlama: An Open-Source Small Language Model},
 url = {http://arxiv.org/pdf/2401.02385},
 file = {Zhang, Zeng et al. 04.01.2024 - TinyLlama An Open-Source Small Language:Attachments/Zhang, Zeng et al. 04.01.2024 - TinyLlama An Open-Source Small Language.pdf:application/pdf}
}


@misc{Zhang.06.04.2023,
 abstract = {Stance detection predicts attitudes towards targets in texts and has gained attention with the rise of social media. Traditional approaches include conventional machine learning, early deep neural networks, and pre-trained fine-tuning models. However, with the evolution of very large pre-trained language models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face deployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not requiring backpropagation training, has emerged as a promising alternative. This paper examines CoT's effectiveness in stance detection tasks, demonstrating its superior accuracy and discussing associated challenges.},
 author = {Zhang, Bowen and Fu, Xianghua and Ding, Daijun and Huang, Hu and Dai, Genan and Yin, Nan and Li, Yangyang and Jing, Liwen},
 date = {06.04.2023},
 title = {Investigating Chain-of-thought with ChatGPT for Stance Detection on  Social Media},
 url = {http://arxiv.org/pdf/2304.03087v2}
}


@inproceedings{Yu.2024,
 abstract = {Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.},
 author = {Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng},
 title = {WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning},
 url = {https://aclanthology.org/2024.acl-long.280},
 pages = {5140--5153},
 publisher = {{Association for Computational Linguistics}},
 editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2024},
 address = {Bangkok, Thailand},
 doi = {10.18653/v1/2024.acl-long.280}
}


@inproceedings{Yin.2018,
 author = {Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
 title = {Learning to mine aligned code and natural language pairs from stack overflow},
 pages = {476--486},
 booktitle = {2018 IEEE/ACM 15th international conference on mining software repositories (MSR)},
 year = {2018}
}


@article{Yang.2024,
 author = {Yang and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Dong, Guanting and Wei, Haoran and Lin, Huan and Tang, Jialong and Wang, Jialin and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Ma, Jianxin and Xu, Jin and Zhou, Jingren and Bai, Jinze and He, Jinzheng and Lin, Junyang and Dang, Kai and Lu, Keming and Chen, Keqin and Yang, Kexin and Li, Mei and Xue, Mingfeng and Ni, Na and Zhang, Pei and Wang, Peng and Peng, Ru and Men, Rui and Gao, Ruize and Lin, Runji and Wang, Shijie and Bai, Shuai and Tan, Sinan and Zhu, Tianhang and Li, Tianhao and Liu, Tianyu and Ge, Wenbin and Deng, Xiaodong and Zhou, Xiaohuan and Ren, Xingzhang and Zhang, Xinyu and Wei, Xipin and Ren, Xuancheng and Fan, Yang and Yao, Yang and Zhang, Yichang and Wan, Yu and Chu, Yunfei and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Fan, Zhihao},
 year = {2024},
 title = {Qwen2 Technical Report},
 journal = {arXiv preprint arXiv:2407.10671}
}


@misc{Xu.19.12.2023,
 abstract = {With the continuous growth in the number of parameters of transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, we conduct experiments using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.},
 author = {Xu, Lingling and Xie, Haoran and Qin, Si-Zhao Joe and Tao, Xiaohui and Wang, Fu Lee},
 date = {19.12.2023},
 title = {Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models:  A Critical Review and Assessment},
 url = {http://arxiv.org/pdf/2312.12148},
 file = {Xu, Xie et al. 19.12.2023 - Parameter-Efficient Fine-Tuning Methods for Pretrained:Attachments/Xu, Xie et al. 19.12.2023 - Parameter-Efficient Fine-Tuning Methods for Pretrained.pdf:application/pdf}
}


@inproceedings{Xu.2024,
 author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin},
 title = {WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions},
 url = {https://openreview.net/forum?id=CfXh93NDgH},
 booktitle = {The Twelfth International Conference on Learning Representations},
 year = {2024}
}


@misc{Wu.2016,
 abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ({\textquotedbl}wordpieces{\textquotedbl}) for both input and output. This method provides a good balance between the flexibility of {\textquotedbl}character{\textquotedbl}-delimited models and the efficiency of {\textquotedbl}word{\textquotedbl}-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
 author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and {Le V}, Quoc and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
 date = {2016},
 title = {Google's Neural Machine Translation System: Bridging the Gap between  Human and Machine Translation},
 url = {http://arxiv.org/pdf/1609.08144},
 file = {Wu, Schuster et al. 26.09.2016 - Google's Neural Machine Translation System:Attachments/Wu, Schuster et al. 26.09.2016 - Google's Neural Machine Translation System.pdf:application/pdf}
}


@article{Winterer.2020,
 abstract = {We propose type-aware operator mutation, a simple, but unusually effective approach for testing SMT solvers. The key idea is to mutate operators of conforming types within the seed formulas to generate well-typed mutant formulas. These mutant formulas are then used as the test cases for SMT solvers. We realized type-aware operator mutation within the OpFuzz tool and used it to stress-test Z3 and CVC4, two state-of-the-art SMT solvers. Type-aware operator mutations are unusually effective: During one year of extensive testing with OpFuzz, we reported 1092 bugs on Z3's and CVC4's respective GitHub issue trackers, out of which 819 unique bugs were confirmed and 685 of the confirmed bugs were fixed by the developers. The detected bugs are highly diverse --- we found bugs of many different types (soundness bugs, invalid model bugs, crashes, etc.), logics and solver configurations. We have further conducted an in-depth study of the bugs found by OpFuzz. The study results show that the bugs found by OpFuzz are of high quality. Many of them affect core components of the SMT solvers' codebases, and some required major changes for the developers to fix. Among the 819 confirmed bugs found by OpFuzz,184 were soundness bugs, the most critical bugs in SMT solvers,and 489 were in the default modes of the solvers. Notably, OpFuzz found 27 critical soundness bugs in CVC4, which has proved to be a very stable SMT solver.},
 author = {Winterer, Dominik and Zhang, Chengyu and Su, Zhendong},
 year = {2020},
 title = {On the unusual effectiveness of type-aware operator mutations for testing SMT solvers},
 keywords = {Fuzz testing;SMT solvers;Type-aware operator mutation},
 volume = {4},
 number = {OOPSLA},
 journal = {Proceedings of the ACM on Programming Languages},
 doi = {10.1145/3428261}
}


@misc{Weyssow.21.08.2023,
 abstract = {Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In this paper, we deliver a comprehensive study of PEFT techniques for LLMs under the automated code generation scenario. Our comprehensive investigation of PEFT techniques for LLMs reveals their superiority and potential over ICL across a diverse set of LLMs. Additionally, we demonstrate the extended capabilities of PEFT, showcasing its ability to learn from two distinct datasets jointly without compromising performance. Furthermore, our study highlights the potential for tuning larger LLMs and significant reductions in memory usage by combining PEFT with quantization. Therefore, this study opens opportunities for broader applications of PEFT in software engineering scenarios. Our code is available at https://github.com/martin-wey/peft-llm-code/.},
 author = {Weyssow, Martin and Zhou, Xin and Kim, Kisub and {Lo David} and Sahraoui, Houari},
 date = {21.08.2023},
 title = {Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation  with Large Language Models}
}


@article{Weizenbaum.1966,
 author = {Weizenbaum, Joseph},
 year = {1966},
 title = {ELIZA---a computer program for the study of natural language communication between man and machine},
 pages = {36--45},
 volume = {9},
 number = {1},
 issn = {0001-0782},
 journal = {Communications of the ACM},
 doi = {10.1145/365153.365168}
}


@article{Wei.2023,
 author = {Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and Li, Chenxia and Yang, Liu and Luo, Xilin and Wu, Xuejie and Liu, Lunan and Cheng, Wenjun and Cheng, Peng and Zhang, Jianhao and Zhang, Xiaoyu and Lin, Lei and Wang, Xiaokun and Ma, Yutuan and Dong, Chuanhai and Sun, Yanqi and Chen, Yifu and Peng, Yongyi and Liang, Xiaojuan and Yan, Shuicheng and Fang, Han and Zhou, Yahui},
 year = {2023},
 title = {Skywork: A More Open Bilingual Foundation Model},
 keywords = {dblp},
 volume = {abs/2310.19341},
 journal = {CoRR}
}


@inproceedings{Wei.2024,
 abstract = {We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using {\textless}b{\textgreater}OSS-Instruct{\textless}/b{\textgreater}, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.},
 author = {Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
 title = {Magicoder: Empowering Code Generation with OSS-Instruct},
 url = {https://proceedings.mlr.press/v235/wei24h.html},
 pages = {52632--52657},
 volume = {235},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 year = {2024}
}


@inproceedings{Wang.2018,
 abstract = {Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.},
 author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 title = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
 url = {https://aclanthology.org/W18-5446},
 pages = {353--355},
 publisher = {{Association for Computational Linguistics}},
 editor = {Linzen, Tal and Chrupa{\l}a, Grzegorz and Alishahi, Afra},
 booktitle = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
 year = {2018},
 address = {Brussels, Belgium},
 doi = {10.18653/v1/W18-5446}
}


@misc{Soboleva.09.06.2023,
 author = {Soboleva, D. and Al-Khateeb, F. and Myers, R. and Steeves, J. R. and Hestness, J. and Dey, N.},
 date = {09.06.2023},
 title = {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.},
 url = {https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama},
 urldate = {01.09.2024},
 institution = {{Cerebras Inference}}
}


@inproceedings{Wang.2022,
 abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions---training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9{\%} on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
 author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Pal, Kuntal Kumar and Patel, Maitreya and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Doshi, Savan and Sampat, Shailaja Keyur and Mishra, Siddhartha and {Reddy A}, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong},
 title = {Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks},
 url = {https://aclanthology.org/2022.emnlp-main.340},
 pages = {5085--5109},
 publisher = {{Association for Computational Linguistics}},
 editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 year = {2022},
 address = {Abu Dhabi, United Arab Emirates},
 doi = {10.18653/v1/2022.emnlp-main.340}
}


@inproceedings{Wang.2020,
 author = {Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
 title = {Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree},
 pages = {261--271},
 booktitle = {2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
 year = {2020}
}


@article{Wang.2018b,
 author = {Wang, Xinyu and Dillig, Isil and Singh, Rishabh},
 year = {2018},
 title = {Program synthesis using abstraction refinement},
 pages = {1--30},
 volume = {2},
 number = {POPL},
 journal = {Proceedings of the ACM on Programming Languages},
 file = {3158151:Attachments/3158151.pdf:application/pdf}
}


@misc{Wang.20.09.2023,
 abstract = {Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat and https://huggingface.co/openchat.},
 author = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
 date = {20.09.2023},
 title = {OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
 url = {http://arxiv.org/pdf/2309.11235},
 file = {Wang, Cheng et al. 20.09.2023 - OpenChat Advancing Open-source Language Models:Attachments/Wang, Cheng et al. 20.09.2023 - OpenChat Advancing Open-source Language Models.pdf:application/pdf}
}


@proceedings{Wallach.2019,
 year = {2019},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{Curran Associates, Inc}},
 editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d' Alch{\'e}-Buc, F. and Fox, E. and Garnett, R.}
}


@inproceedings{Vaswani.2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, $\backslash$L ukasz and Polosukhin, Illia},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. Von Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2017}
}


@proceedings{Vanderwende.2013,
 year = {2013},
 title = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 address = {Atlanta, Georgia},
 publisher = {{Association for Computational Linguistics}},
 editor = {Vanderwende, Lucy and {Daum{\'e} III}, Hal and Kirchhoff, Katrin}
}


@misc{Turgutlu.22.04.2024,
 author = {Turgutlu, Kerem},
 year = {22.04.2024},
 title = {Efficient finetuning of Llama 3 with FSDP QDoRA},
 url = {https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html},
 urldate = {08.10.2024}
}


@article{Tieleman.2012,
 author = {Tieleman, T. and Hinton, G.},
 year = {2012},
 title = {Lecture 6.5 - RMSProp: Divide the Gradient by a Running Average of Its Recent Magnitude},
 pages = {26--31},
 volume = {4},
 journal = {COURSERA: Neural Networks for Machine Learning}
}


@proceedings{.2021,
 year = {2021},
 title = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}
}


@proceedings{.2024b,
 year = {2024},
 title = {The Twelfth International Conference on Learning Representations}
}


@proceedings{.2024c,
 year = {2024},
 title = {The Twelfth International Conference on Learning Representations}
}


@proceedings{.2024d,
 year = {2024},
 title = {The Twelfth International Conference on Learning Representations}
}


@proceedings{.2023,
 year = {2023},
 title = {The Eleventh International Conference on Learning Representations}
}


@inproceedings{Svajlenko.2014,
 author = {Svajlenko, Jeffrey and Islam, Judith F. and Keivanloo, Iman and Roy, Chanchal K. and Mia, Mohammad Mamun},
 title = {Towards a big data curated benchmark of inter-project code clones},
 pages = {476--480},
 booktitle = {2014 IEEE International Conference on Software Maintenance and Evolution},
 year = {2014}
}


@inproceedings{Wang.2023,
 abstract = {Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model's robustness performance. With human annotators, we verified that over 90{\%} of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval.},
 author = {Wang, Shiqi and Li, Zheng and Qian, Haifeng and Yang, Chenghao and Wang, Zijian and Shang, Mingyue and Kumar, Varun and Tan, Samson and Ray, Baishakhi and Bhatia, Parminder and Nallapati, Ramesh and Ramanathan, Murali Krishna and Roth, Dan and Xiang, Bing},
 title = {ReCode: Robustness Evaluation of Code Generation Models},
 url = {https://aclanthology.org/2023.acl-long.773},
 pages = {13818--13843},
 publisher = {{Association for Computational Linguistics}},
 editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 year = {2023},
 address = {Toronto, Canada},
 doi = {10.18653/v1/2023.acl-long.773}
}


@misc{Ling.22.03.2016,
 abstract = {Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.},
 author = {Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}} and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
 date = {22.03.2016},
 title = {Latent Predictor Networks for Code Generation},
 url = {http://arxiv.org/pdf/1603.06744v2}
}


@misc{Liang.06.04.2023,
 abstract = {The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversation about the ethical implications of deploying ChatGPT content detectors and caution against their use in evaluative or educational settings, particularly when they may inadvertently penalize or exclude non-native English speakers from the global discourse. The published version of this study can be accessed at: www.cell.com/patterns/fulltext/S2666-3899(23)00130-7},
 author = {Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
 date = {06.04.2023},
 title = {GPT detectors are biased against non-native English writers},
 url = {http://arxiv.org/pdf/2304.02819v3}
}


@article{Li.2023,
 author = {Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and {Del Giorno}, Allie and Gunasekar, Suriya and Lee, Yin Tat},
 year = {2023},
 title = {Textbooks Are All You Need II: phi-1.5 technical report},
 journal = {arXiv preprint arXiv:2309.05463}
}


@inproceedings{Gao.2023,
 abstract = {Large language models (LLMs) have demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ({\textquotedbl}few-shot prompting{\textquotedbl}). Much of this success can be attributed to prompting methods such as {\textquotedbl}chain-of-thought{\textquotedbl}, which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and others. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on GSM8K, surpassing PaLM which uses chain-of-thought by absolute 15{\%} top-1.},
 author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
 title = {PAL: Program-aided Language Models},
 url = {https://proceedings.mlr.press/v202/gao23f.html},
 pages = {10764--10799},
 volume = {202},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 year = {2023}
}


@inproceedings{Fried.2023,
 author = {Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Scott and Zettlemoyer, Luke and Lewis, Mike},
 title = {InCoder: A Generative Model for Code Infilling and Synthesis},
 url = {https://openreview.net/forum?id=hQwb-lbM6EL},
 booktitle = {The Eleventh International Conference on Learning Representations},
 year = {2023}
}


@misc{Feng.19.02.2020,
 abstract = {We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.},
 author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
 date = {19.02.2020},
 title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
 url = {http://arxiv.org/pdf/2002.08155},
 file = {Feng, Guo et al. 19.02.2020 - CodeBERT A Pre-Trained Model:Attachments/Feng, Guo et al. 19.02.2020 - CodeBERT A Pre-Trained Model.pdf:application/pdf}
}


@book{Eisenstein.2019,
 abstract = {{\textquotedbl}The book provides a technical perspective on the most contemporary data-driven approaches, focusing on techniques from supervised and unsupervised machine learning. It also includes background in the salient linguistic issues, as well as computational representations and algorithms. The first section of the book explores what can be with individual words. The second section concerns structured representations such as sequences, trees, and graphs. The third section highlights different approaches to the representation and analysis of linguistic meaning. The final section describes three of the most transformative applications of natural language processing: information extraction, machine translation, and text generation. The book describes the technical foundations of the field, including the most relevant machine learning techniques, algorithms, and linguistic representations. From these foundations, it extends to contemporary research in areas such as deep learning. Each chapter contains exercises that include paper-and-pencil analysis of the computational algorithms and linguistic issues, as well as software implementations{\textquotedbl}--},
 author = {Eisenstein, Jacob},
 year = {2019},
 title = {Introduction to natural language processing},
 address = {Cambridge Massachusetts},
 publisher = {{The MIT Press}},
 isbn = {9780262042840},
 series = {Adaptive computation and machine learning},
 file = {eisenstein-natural-language-processing:Attachments/eisenstein-natural-language-processing.pdf:application/pdf}
}


@article{Duchi.2011,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 year = {2011},
 title = {Adaptive subgradient methods for online learning and stochastic optimization},
 url = {https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
 pages = {2121--2159},
 volume = {12},
 journal = {Journal of Machine Learning Research},
 file = {Adaptive subgradient methods for online 2011:Attachments/Adaptive subgradient methods for online 2011.pdf:application/pdf}
}


@article{Du.2024,
 author = {Du, Mingzhe and Luu, Anh Tuan and Ji, Bin and Ng, See-Kiong},
 year = {2024},
 title = {Mercury: An Efficiency Benchmark for LLM Code Synthesis},
 journal = {arXiv preprint arXiv:2402.07844}
}


@inproceedings{Devlin.2019,
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://api.semanticscholar.org/CorpusID:52967399},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 year = {2019}
}


@inproceedings{Dettmers.2023,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 title = {QLoRA: Efficient Finetuning of Quantized LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
 pages = {10088--10115},
 volume = {36},
 publisher = {{Curran Associates, Inc}},
 editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2023}
}


@misc{Dettmers.06.10.2021,
 abstract = {Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradient values. This state can be used to accelerate optimization compared to plain stochastic gradient descent but uses memory that might otherwise be allocated to model parameters, thereby limiting the maximum size of models trained in practice. In this paper, we develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states. To overcome the resulting computational, quantization, and stability challenges, we develop block-wise dynamic quantization. Block-wise quantization divides input tensors into smaller blocks that are independently quantized. Each block is processed in parallel across cores, yielding faster optimization and high precision quantization. To maintain stability and performance, we combine block-wise quantization with two additional changes: (1) dynamic quantization, a form of non-linear optimization that is precise for both large and small magnitude values, and (2) a stable embedding layer to reduce gradient variance that comes from the highly non-uniform distribution of input tokens in language models. As a result, our 8-bit optimizers maintain 32-bit performance with a small fraction of the memory footprint on a range of tasks, including 1.5B parameter language modeling, GLUE finetuning, ImageNet classification, WMT'14 machine translation, MoCo v2 contrastive ImageNet pretraining+finetuning, and RoBERTa pretraining, without changes to the original optimizer hyperparameters. We open-source our 8-bit optimizers as a drop-in replacement that only requires a two-line code change.},
 author = {Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
 date = {06.10.2021},
 title = {8-bit Optimizers via Block-wise Quantization},
 url = {http://arxiv.org/pdf/2110.02861},
 file = {Dettmers, Lewis et al. 06.10.2021 - 8-bit Optimizers via Block-wise Quantization:Attachments/Dettmers, Lewis et al. 06.10.2021 - 8-bit Optimizers via Block-wise Quantization.pdf:application/pdf}
}


@misc{DeepSeekAI.2024,
 abstract = {We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.},
 author = {DeepSeek-AI and Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y. and Li, Yukun and Gao, Huazuo and Ma, Shirong and Zeng, Wangding and Bi, Xiao and Gu, Zihui and Xu, Hanwei and Dai, Damai and Dong, Kai and Zhang, Liyue and Piao, Yishi and Gou, Zhibin and Xie, Zhenda and Hao, Zhewen and Wang, Bingxuan and Song, Junxiao and Chen, Deli and Xie, Xin and Guan, Kang and You, Yuxiang and Liu, Aixin and {Du Qiushi} and Gao, Wenjun and Lu, Xuan and Chen, Qinyu and Wang, Yaohui and Deng, Chengqi and Li, Jiashi and Zhao, Chenggang and Ruan, Chong and Luo, Fuli and Liang, Wenfeng},
 date = {2024},
 title = {DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code  Intelligence},
 url = {http://arxiv.org/pdf/2406.11931v1}
}


@misc{CodeGemmaTeam.2024,
 author = {{CodeGemma Team} and Hartman, Ale Jakse and Hu, Andrea and Choquette-Choo, Christopher A. and Zhao, Heri and Fine, Jane and Hui, Jeffrey and Shen, Jingyue and Kelley, Joe and Howland, Joshua and Bansal, Kshitij and Vilnis, Luke and Wirth, Mateo and Nguyen, Nam and Michel, Paul and Choy, Peter and Joshi, Pratik and Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and Zuo, Siqi and Warkentin, Tris and Gong, Zhitao et al.},
 year = {2024},
 title = {CodeGemma Model},
 url = {https://www.kaggle.com/models/google/codegemma},
 publisher = {Kaggle}
}


@misc{Cobbe.2021,
 abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
 date = {2021},
 title = {Training Verifiers to Solve Math Word Problems},
 url = {http://arxiv.org/pdf/2110.14168},
 file = {Cobbe, Kosaraju et al. 27.10.2021 - Training Verifiers to Solve Math:Attachments/Cobbe, Kosaraju et al. 27.10.2021 - Training Verifiers to Solve Math.pdf:application/pdf}
}


@book{Chowdhary.2020,
 abstract = {Fundamentals of Artificial Intelligence introduces the foundations of present day AI and provides coverage to recent developments in AI such as Constraint Satisfaction Problems, Adversarial Search and Game Theory, Statistical Learning Theory, Automated Planning, Intelligent Agents, Information Retrieval, Natural Language {\&} Speech Processing, and Machine Vision. The book features a wealth of examples and illustrations, and practical approaches along with the theoretical concepts. It covers all major areas of AI in the domain of recent developments. The book is intended primarily for students who major in computer science at undergraduate and graduate level but will also be of interest as a foundation to researchers in the area of AI.},
 year = {2020},
 title = {Fundamentals of Artificial Intelligence},
 address = {New Delhi},
 edition = {1st ed. 2020},
 publisher = {{Springer India}},
 isbn = {978-81-322-3972-7},
 editor = {Chowdhary, K.R}
}


@inproceedings{Cho.2014,
 author = {Cho, Kyunghyun and {van Merri{\"e}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
 title = {Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
 url = {https://aclanthology.org/D14-1179},
 pages = {1724--1734},
 publisher = {{Association for Computational Linguistics}},
 editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
 booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 year = {2014},
 address = {Doha, Qatar},
 doi = {10.3115/v1/D14-1179}
}


@misc{GemmaTeam.2024,
 author = {{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and al, et},
 date = {2024},
 title = {Gemma},
 url = {https://www.kaggle.com/m/3301},
 urldate = {24.10.2024},
 institution = {Google},
 doi = {10.34740/KAGGLE/M/3301}
}


@misc{Chiang.2023,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
 year = {2023},
 title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90{\%}* ChatGPT Quality},
 url = {https://lmsys.org/blog/2023-03-30-vicuna/}
}


@misc{Chen.07.07.2021,
 abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8{\%} of the problems, while GPT-3 solves 0{\%} and GPT-J solves 11.4{\%}. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2{\%} of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
 author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and {Pinto, Henrique Ponde de Oliveira} and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
 date = {07.07.2021},
 title = {Evaluating Large Language Models Trained on Code},
 url = {http://arxiv.org/pdf/2107.03374},
 file = {Chen, Tworek et al. 07.07.2021 - Evaluating Large Language Models Trained:Attachments/Chen, Tworek et al. 07.07.2021 - Evaluating Large Language Models Trained.pdf:application/pdf}
}


@article{Cassano.2023,
 author = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q. and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
 year = {2023},
 title = {MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
 keywords = {B.2.3 reliability;Benchmark testing;Codes;fault-tolerance;I.5.1.D neural nets;Natural languages;Programming;Python;Syntactics;Task analysis;testing},
 pages = {3675--3691},
 volume = {49},
 number = {7},
 journal = {IEEE Transactions on Software Engineering},
 doi = {10.1109/TSE.2023.3267446}
}


@inproceedings{Brown.2020,
 abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 title = {Language models are few-shot learners},
 publisher = {{Curran Associates Inc}},
 isbn = {9781713829546},
 series = {NIPS '20},
 booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
 year = {2020},
 address = {Red Hook, NY, USA}
}


@proceedings{Bengio.2015,
 year = {2015},
 title = {3rd International Conference on Learning Representations, ICLR 2015,  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 editor = {Bengio, Yoshua and LeCun, Yann}
}


@article{Bengio.2003,
 abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 year = {2003},
 title = {A neural probabilistic language model},
 pages = {1137--1155},
 volume = {3},
 issn = {1532-4435},
 journal = {J. Mach. Learn. Res.}
}


@misc{BenAllal.2022,
 author = {{Ben Allal}, Loubna and Muennighoff, Niklas and {Kumar Umapathi}, Logesh and Lipkin, Ben and von Werra, Leandro},
 year = {2022},
 title = {A framework for the evaluation of code generation models},
 publisher = {GitHub},
 journal = {GitHub repository}
}


@misc{BaptisteRoziereJonasGehringFabianGloeckleStenSootlaItaiGatXiaoqingEllenTanYossi.24.08.2023,
 author = {{Baptiste Rozi{\`e}re, Jonas Gehring, Fabian Gloeckle,, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J{\'e}r{\'e}my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D{\'e}fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve}},
 date = {24.08.2023},
 title = {Code Llama: Open Foundation Models for Code},
 url = {https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/},
 urldate = {24.10.2024},
 institution = {{Meta AI}}
}


@misc{Bai.2023,
 abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.},
 author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and Hui, Binyuan and Ji, Luo and Li, Mei and Lin, Junyang and Lin, Runji and Liu, Dayiheng and Liu, Gao and Lu, Chengqiang and Lu, Keming and Ma, Jianxin and Men, Rui and Ren, Xingzhang and Ren, Xuancheng and Tan, Chuanqi and Tan, Sinan and Tu, Jianhong and Wang, Peng and Wang, Shijie and Wang, Wei and Wu, Shengguang and Xu, Benfeng and Xu, Jin and Yang and Yang, Hao and Yang, Jian and Yang, Shusheng and Yao, Yang and Yu, Bowen and Yuan, Hongyi and Yuan, Zheng and Zhang, Jianwei and Zhang, Xingxuan and Zhang, Yichang and Zhang, Zhenru and Zhou, Chang and Zhou, Jingren and Zhou, Xiaohuan and Zhu, Tianhang},
 date = {2023},
 title = {Qwen Technical Report},
 url = {http://arxiv.org/pdf/2309.16609v1},
 file = {Bai, Bai et al. 28.09.2023 - Qwen Technical Report:Attachments/Bai, Bai et al. 28.09.2023 - Qwen Technical Report.pdf:application/pdf}
}


@misc{Alon.04.08.2018,
 abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present {\$}{\rm {\scriptsize CODE2SEQ}}{\$}: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to $16$M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
 author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
 date = {04.08.2018},
 title = {code2seq: Generating Sequences from Structured Representations of Code},
 url = {http://arxiv.org/pdf/1808.01400v6}
}


@article{Allal.2023,
 author = {Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Mu{\~n}oz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and Umapathi, Logesh Kumar and Anderson, Carolyn Jane and Zi, Yangtian and Lamy-Poirier, Joel and Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry and Romero, Manuel and Lappert, Michael and de Toni, Francesco and {Del R{\'i}o}, Bernardo Garc{\'i}a and Liu, Qian and Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue and Yu, Ian and Villegas, Paulo and Zocca, Marco and Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau, Dzmitry and Jernite, Yacine and Hughes, Sean and Fried, Daniel and Guha, Arjun and de Vries, Harm and von Werra, Leandro},
 year = {2023},
 title = {SantaCoder: don't reach for the stars!},
 keywords = {dblp},
 volume = {abs/2301.03988},
 journal = {CoRR}
}


@misc{AlecRadfordKarthikNarasimhanTimSalimansIlyaSutskever.2018,
 author = {{Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever}},
 date = {2018},
 title = {Improving Language Understanding by Generative Pre-Training},
 url = {https://openai.com/index/language-unsupervised/},
 urldate = {24.10.2024},
 institution = {OpenAI},
 file = {language{\_}understanding{\_}paper:Attachments/language{\_}understanding{\_}paper.pdf:application/pdf}
}


@proceedings{.2019b,
 year = {2019},
 title = {Advances in Neural Information Processing Systems}
}


@proceedings{.,
 title = {Advances in Neural Information Processing Systems}
}


@misc{Abdin.2024,
 abstract = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69{\%} on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75{\%} and 78{\%} on MMLU, and 8.7 and 8.9 on MT-bench).},
 author = {Abdin, Marah I. and {Ade Jacobs}, Sam and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and {Hassan Awadalla}, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and Benhaim, Alon and Bilenko, Misha and Bjorck, Johan and Bubeck, S{\'e}bastien and Cai, Martin and Mendes, Caio C{\'e}sar Teodoro and Chen, Weizhu and Chaudhary, Vishrav and Chopra, Parul and {Del Giorno}, Allie and de Rosa, Gustavo and Dixon, Matthew and Eldan, Ronen and Iter, Dan and Goswami, Abhishek and Gunasekar, Suriya and Haider, Emman and Hao, Junheng and {Russell J. Hewett} and Huynh, Jamie and Javaheripi, Mojan and Jin, Xin and Kauffmann, Piero and Karampatziakis, Nikos and Kim, Dongwoo and Khademi, Mahmoud and Kurilenko, Lev and Lee, James R. and Lee, Yin Tat and Li, Yuanzhi and Liang, Chen and Liu, Weishung and Lin, Xihui and Lin, Zeqi and Madan, Piyush and Mitra, Arindam and Modi, Hardik and Nguyen, Anh and Norick, Brandon and Patra, Barun and Perez-Becker, Daniel and Portet, Thomas and Pryzant, Reid and Qin, Heyang and Radmilac, Marko and Rosset, Corby and Roy, Sambudha and Saarikivi, Olli and Saied, Amin and Salim, Adil and Santacroce, Michael and Shah, Shital and Shang, Ning and Sharma, Hiteshi and Song, Xia and Ruwase, Olatunji and Wang, Xin and Ward, Rachel and Wang, Guanhua and Witte, Philipp and Wyatt, Michael and Xu, Can and Xu, Jiahang and Xu, Weijian and Yadav, Sonali and Yang, Fan and Yang, Ziyi and Yu, Donghan and Zhang, Chengruidong and Zhang, Cyril and Zhang, Jianwen and Zhang, Li Lyna and Zhang, Yi and Zhang, Yunan and Zhou, Xiren},
 date = {2024},
 title = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
 url = {https://www.microsoft.com/en-us/research/publication/phi-3-technical-report-a-highly-capable-language-model-locally-on-your-phone/},
 number = {MSR-TR-2024-12},
 institution = {Microsoft}
}


@misc{Chen.2021,
 author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and {Henrique Ponde de Oliveira Pinto} and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
 date = {2021},
 title = {Evaluating Large Language Models Trained on Code},
 url = {http://arxiv.org/pdf/2107.03374}
}


@misc{GitHub.20240621,
 author = {GitHub},
 year = {2024-06-21},
 title = {Programming languages: Top 50 Programming Languages Globally},
 url = {https://innovationgraph.github.com/global-metrics/programming-languages}
}


@proceedings{Goldberg.2022,
 year = {2022},
 title = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 address = {Abu Dhabi, United Arab Emirates},
 publisher = {{Association for Computational Linguistics}},
 editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue}
}


@misc{Gunasekar.2023,
 abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of {\grqq}textbook quality{\textquotedbl} data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6{\%} on HumanEval and 55.5{\%} on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45{\%} on HumanEval.},
 author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Cesar, Caio and Mendes, Teodoro and {Del Giorno}, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and {Singh Behl}, Harkirat and Wang, Xin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
 year = {2023},
 title = {Textbooks Are All You Need},
 url = {https://www.microsoft.com/en-us/research/publication/textbooks-are-all-you-need/}
}


@article{Li.2023b,
 author = {Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and Liu, Qian and Zheltonozhskii, Evgenii and Zhuo, Terry Yue and Wang, Thomas and Dehaene, Olivier and Lamy-Poirier, Joel and Monteiro, Joao and Gontier, Nicolas and Yee, Ming-Ho and Umapathi, Logesh Kumar and Zhu, Jian and Lipkin, Ben and Oblokulov, Muhtasham and Wang, Zhiruo and Murthy, Rudra and Stillerman, Jason T. and Patel, Siva Sankalp and Abulkhanov, Dmitry and Zocca, Marco and Dey, Manan and Zhang, Zhihan and Bhattacharyya, Urvashi and Yu, Wenhao and Luccioni, Sasha and Villegas, Paulo and Zhdanov, Fedor and Lee, Tony and Timor, Nadav and Ding, Jennifer and Schlesinger, Claire S. and Schoelkopf, Hailey and Ebert, Jan and Dao, Tri and Mishra, Mayank and Gu, Alex and Anderson, Carolyn Jane and Dolan-Gavitt, Brendan and Contractor, Danish and Reddy, Siva and Fried, Daniel and Bahdanau, Dzmitry and Jernite, Yacine and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm},
 year = {2023},
 title = {StarCoder: may the source be with you!},
 url = {https://openreview.net/forum?id=KoFOg41haE},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research}
}


@misc{Lewis.22.05.2020,
 abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 date = {22.05.2020},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {http://arxiv.org/pdf/2005.11401v4},
 file = {Lewis, Perez et al 22052020 - Retrieval-Augmented Generation for Knowledge-Intensive NLP:Attachments/Lewis, Perez et al 22052020 - Retrieval-Augmented Generation for Knowledge-Intensive NLP.pdf:application/pdf}
}


@article{Lees.1957,
 author = {Lees, Robert B. and Chomsky, Noam},
 year = {1957},
 title = {Syntactic Structures},
 pages = {375},
 volume = {33},
 number = {3},
 issn = {00978507},
 journal = {Language},
 doi = {10.2307/411160}
}


@article{Lecun.1998,
 author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
 year = {1998},
 title = {Gradient-based learning applied to document recognition},
 keywords = {Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis},
 pages = {2278--2324},
 volume = {86},
 number = {11},
 journal = {Proceedings of the IEEE}
}


@inproceedings{Lai.2023,
 abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as Numpy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accepts, only 1.8{\%} of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3{\%} accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
 author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
 title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
 url = {https://proceedings.mlr.press/v202/lai23b.html},
 pages = {18319--18345},
 volume = {202},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
 booktitle = {Proceedings of the 40th International Conference on Machine Learning},
 year = {2023}
}


@inproceedings{Kumar.2017,
 author = {Kumar, Anjishnu and Gupta, Arpit and Chan, Julian and Tucker, Sam and Hoffmeister, Bj{\"o}rn and Dreyer, Markus and Peshterliev, Stanislav and Gandhe, Ankur and Filimonov, Denis and Rastrow, Ariya and Monson, Christian and Kumar, Agnika},
 title = {Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding},
 url = {https://www.amazon.science/publications/just-ask-building-an-architecture-for-extensible-self-service-spoken-language-understanding},
 booktitle = {NeurIPS 2017},
 year = {2017}
}


@inproceedings{Kulal.2019,
 author = {Kulal, Sumith and Pasupat, Panupong and Chandra, Kartik and Lee, Mina and Padon, Oded and Aiken, Alex and Liang, Percy S.},
 title = {SPoC: Search-based Pseudocode to Code},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf},
 volume = {32},
 publisher = {{Curran Associates, Inc}},
 editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d' Alch{\'e}-Buc, F. and Fox, E. and Garnett, R.},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2019}
}


@proceedings{Ku.2024,
 year = {2024},
 title = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 address = {Bangkok, Thailand},
 publisher = {{Association for Computational Linguistics}},
 editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}
}


@proceedings{Krause.2023,
 year = {2023},
 title = {Proceedings of the 40th International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}
}


@proceedings{Krause.2023b,
 year = {2023},
 title = {Proceedings of the 40th International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}
}


@article{Kocetkov.2023,
 author = {Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Jernite, Yacine and Mitchell, Margaret and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
 year = {2023},
 title = {The Stack: 3 TB of permissively licensed source code},
 url = {https://openreview.net/forum?id=pxpbTdUEpD},
 issn = {2835-8856},
 journal = {Transactions on Machine Learning Research}
}


@inproceedings{Kingma.2015,
 author = {Kingma, Diederik P. and Ba, Jimmy},
 title = {Adam: A Method for Stochastic Optimization},
 editor = {Bengio, Yoshua and LeCun, Yann},
 booktitle = {3rd International Conference on Learning Representations, ICLR 2015,  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 year = {2015}
}


@book{Jurafsky.2024,
 author = {Jurafsky, Daniel and Martin, James H.},
 year = {2024},
 title = {Speech and Language Processing: An Introduction to  Natural Language Processing, Computational Linguistics,  and Speech Recognition with Language Models},
 edition = {3rd Edition}
}


@article{Johri.2021,
 abstract = {It is quite hard to imagine a smart system like a voice assistant or a chat-bot or a recommender system without natural language processing (NLP). It all starts with an initial unit that first interprets the data (audio or text) provided and then start making sense...},
 author = {Johri, Prashant and Khatri, Sunil K. and Al-Taani, Ahmad T. and Sabharwal, Munish and Suvanov, Shakhzod and Kumar, Avneesh},
 year = {2021},
 title = {Natural Language Processing: History, Evolution, Application, and Future Work},
 url = {https://link.springer.com/chapter/10.1007/978-981-15-9712-1_31},
 pages = {365--375},
 volume = {167},
 issn = {2367-3389},
 journal = {Proceedings of 3rd International Conference on Computing Informatics and Networks},
 doi = {10.1007/978-981-15-9712-1{\textunderscore }31},
 file = {Johri, Khatri et al. 2021 - Natural Language Processing:Attachments/Johri, Khatri et al. 2021 - Natural Language Processing.pdf:application/pdf}
}


@misc{Jiang.01.06.2024,
 abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.},
 author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
 date = {01.06.2024},
 title = {A Survey on Large Language Models for Code Generation},
 url = {http://arxiv.org/pdf/2406.00515}
}


@misc{Jiang.01.06.2024b,
 abstract = {Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the widely recognized HumanEval and MBPP benchmarks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource website (https://codellm.github.io) to continuously document and disseminate the most recent advances in the field.},
 author = {Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
 date = {01.06.2024},
 title = {A Survey on Large Language Models for Code Generation},
 url = {http://arxiv.org/pdf/2406.00515},
 file = {Jiang, Wang et al. 01.06.2024 - A Survey on Large Language:Attachments/Jiang, Wang et al. 01.06.2024 - A Survey on Large Language.pdf:application/pdf}
}


@misc{Jeon.2023,
 author = {Jeon, Mingi and Baik, Seung-Yeop and Hahn, Joonghyuk and Han, Yo-Sub and Ko, Sang-Ki},
 year = {2023},
 title = {Deep Learning-based Source Code Complexity Prediction},
 url = {https://openreview.net/forum?id=9irBKvxsw9}
}


@misc{Guo.2024,
 abstract = {The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.},
 author = {Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y. and Li, Y. K. and Luo, Fuli and Xiong, Yingfei and Liang, Wenfeng},
 date = {2024},
 title = {DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence},
 url = {http://arxiv.org/pdf/2401.14196v2},
 file = {Guo, Zhu et al 25012024 - DeepSeek-Coder:Attachments/Guo, Zhu et al 25012024 - DeepSeek-Coder.pdf:application/pdf}
}


@inproceedings{He.2010,
 author = {He, Qinlu and Li, Zhanhuai and Zhang, Xiao},
 title = {Data deduplication techniques},
 pages = {430--433},
 publisher = {IEEE},
 isbn = {978-1-4244-9087-5},
 booktitle = {2010 International Conference on Future Information Technology and Management Engineering},
 year = {2010},
 doi = {10.1109/FITME.2010.5656539}
}


@inproceedings{Hendrycks.2021,
 author = {Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
 title = {Measuring Coding Challenge Competence With APPS},
 url = {https://openreview.net/forum?id=sD93GOzH3i5},
 booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
 year = {2021}
}


@inproceedings{Hu.2022,
 author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 url = {https://openreview.net/forum?id=nZeVKeeFYf9},
 booktitle = {International Conference on Learning Representations},
 year = {2022}
}


@proceedings{I.Guyon.2017,
 year = {2017},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{Curran Associates, Inc}},
 editor = {{I. Guyon} and {U. Von Luxburg} and {S. Bengio} and {H. Wallach} and {R. Fergus} and {S. Vishwanathan} and {R. Garnett}}
}


@proceedings{IEEE.2014,
 year = {2014},
 title = {2014 IEEE International Conference on Software Maintenance and Evolution},
 institution = {IEEE}
}


@misc{Zheng.22.02.2024,
 abstract = {The introduction of large language models has significantly advanced code generation. However, open-source models often lack the execution capabilities and iterative refinement of advanced systems like the GPT-4 Code Interpreter. To address this, we introduce OpenCodeInterpreter, a family of open-source code systems designed for generating, executing, and iteratively refining code. Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions, OpenCodeInterpreter integrates execution and human feedback for dynamic code refinement. Our comprehensive evaluation of OpenCodeInterpreter across key benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6) with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap between open-source code generation models and proprietary systems like GPT-4 Code Interpreter.},
 author = {Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
 date = {22.02.2024},
 title = {OpenCodeInterpreter: Integrating Code Generation with Execution and  Refinement},
 url = {http://arxiv.org/pdf/2402.14658},
 file = {Zheng, Zhang et al. 22.02.2024 - OpenCodeInterpreter:Attachments/Zheng, Zhang et al. 22.02.2024 - OpenCodeInterpreter.pdf:application/pdf}
}


@proceedings{IEEE.2018,
 year = {2018},
 title = {2018 IEEE/ACM 15th international conference on mining software repositories (MSR)},
 institution = {IEEE}
}


@proceedings{.2013,
 year = {2013},
 title = {International Conference on Learning Representations}
}


@proceedings{.2022,
 year = {2022},
 title = {International Conference on Learning Representations}
}


@proceedings{.2022b,
 year = {2022},
 title = {International Conference on Learning Representations}
}


@book{.2012,
 year = {2012},
 title = {Introduction to artificial neural network},
 url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=04d0b6952a4f0c7203577afc9476c2fcab2cba06},
 file = {Introduction to artificial neural network 2012:Attachments/Introduction to artificial neural network 2012.pdf:application/pdf}
}


@proceedings{Isabelle.2001,
 year = {2001},
 title = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - ACL '02},
 address = {Morristown, NJ, USA},
 publisher = {{Association for Computational Linguistics}},
 editor = {Isabelle, Pierre},
 doi = {10.3115/1073083}
}


@inproceedings{Iyer.2018,
 abstract = {Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to ``return the smallest element'' in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.},
 author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
 title = {Mapping Language to Code in Programmatic Context},
 url = {https://aclanthology.org/D18-1192},
 pages = {1643--1652},
 publisher = {{Association for Computational Linguistics}},
 editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 year = {2018},
 address = {Brussels, Belgium},
 doi = {10.18653/v1/D18-1192}
}


@proceedings{IEEE.2020,
 year = {2020},
 title = {2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
 institution = {IEEE}
}


@inproceedings{Zhou.2019,
 author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
 title = {Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks},
 pages = {10197--10207},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2019}
}


